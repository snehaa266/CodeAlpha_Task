{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565574bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a978654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:41<00:00, 240297.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 91678.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:06<00:00, 248107.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tr_data =  datasets.MNIST(root='data',train=True,download=True,transform= ToTensor())\n",
    "te_data = datasets.MNIST(root='data',train=False,download=True,transform= ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baca16f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = tr_data[0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d39e779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "print(tr_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b13e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = tr_data[1]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze())\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef0456c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE =32\n",
    "train_dataloader = DataLoader(tr_data,batch_size =BATCH_SIZE,shuffle=True)\n",
    "test_dataloader = DataLoader(te_data,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169b1ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV1 (nn.Module):\n",
    "    def __init__ (self,input_shape,hidden_units,output_shape):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape,out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units,out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,x: torch.Tensor):\n",
    "        return self.layer_stack(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795e6890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV1(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = FashionMNISTModelV1(input_shape=784,hidden_units=10,output_shape=10)\n",
    "model0.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c04869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "904358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model0.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3510298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "  total_time = end - start\n",
    "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "  return total_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5a8622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "looked at 0/60000samples\n",
      "looked at 12800/60000samples\n",
      "looked at 25600/60000samples\n",
      "looked at 38400/60000samples\n",
      "looked at 51200/60000samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:14<00:28, 14.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.75850 | Test loss: 0.73181, Test acc: 72.33%\n",
      "\n",
      "Epoch: 1\n",
      "looked at 0/60000samples\n",
      "looked at 12800/60000samples\n",
      "looked at 25600/60000samples\n",
      "looked at 38400/60000samples\n",
      "looked at 51200/60000samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:26<00:13, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.72026 | Test loss: 0.70507, Test acc: 73.11%\n",
      "\n",
      "Epoch: 2\n",
      "looked at 0/60000samples\n",
      "looked at 12800/60000samples\n",
      "looked at 25600/60000samples\n",
      "looked at 38400/60000samples\n",
      "looked at 51200/60000samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:38<00:00, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.66079 | Test loss: 0.49733, Test acc: 82.53%\n",
      "\n",
      "Train time on cpu: 38.141 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer()\n",
    "epochs=3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_loss = 0\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        model0.train()\n",
    "        y_pred = model0(X)\n",
    "        loss = loss_fn(y_pred,y)\n",
    "        train_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"looked at {batch * len(X)}/{len(train_dataloader.dataset)}samples\")\n",
    "    train_loss /= len(train_dataloader)\n",
    "    test_loss,test_acc = 0, 0 \n",
    "    model0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in test_dataloader:\n",
    "            test_pred=model0(X)\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model0.parameters()).device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a86c62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_custom_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts any image into MNIST-compatible tensor:\n",
    "    - Handles RGB / Grayscale\n",
    "    - Resizes to 28x28\n",
    "    - Auto-inverts background if needed\n",
    "    - Normalizes properly\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    image = ImageOps.grayscale(image)\n",
    "\n",
    "    # Resize to MNIST dimensions\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    # Auto-invert if background is white\n",
    "    # MNIST digits are white on black background\n",
    "    if image_tensor.mean() > 0.5:\n",
    "        image_tensor = 1.0 - image_tensor\n",
    "\n",
    "    # Normalize (MNIST stats)\n",
    "    image_tensor = transforms.Normalize(\n",
    "        mean=(0.1307,),\n",
    "        std=(0.3081,)\n",
    "    )(image_tensor)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2eb1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "\n",
    "# Redefine preprocess_custom_image within this cell to include the necessary imports\n",
    "# (This is done because the original function definition in KpiQfnpEpAdR was missing these imports,\n",
    "# and we are constrained to modify only this cell XGcc3_SK5yhZ.)\n",
    "def preprocess_custom_image(image_path):\n",
    "    \"\"\"\n",
    "    Converts any image into MNIST-compatible tensor:\n",
    "    - Handles RGB / Grayscale\n",
    "    - Resizes to 28x28\n",
    "    - Auto-inverts background if needed\n",
    "    - Normalizes properly\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    image = ImageOps.grayscale(image)\n",
    "\n",
    "    # Resize to MNIST dimensions\n",
    "    image = image.resize((28, 28))\n",
    "\n",
    "    # Convert to tensor\n",
    "    image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    # Auto-invert if background is white\n",
    "    # MNIST digits are white on black background\n",
    "    if image_tensor.mean() > 0.5:\n",
    "        image_tensor = 1.0 - image_tensor\n",
    "\n",
    "    # Normalize (MNIST stats)\n",
    "    image_tensor = transforms.Normalize(\n",
    "        mean=(0.1307,),\n",
    "        std=(0.3081,)\n",
    "    )(image_tensor)\n",
    "\n",
    "    # Add batch dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/content/192869.png\"  # change this\n",
    "\n",
    "image_tensor = preprocess_custom_image(image_path)\n",
    "\n",
    "print(\"Processed image shape:\", image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe26ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_tensor.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"Preprocessed Image\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits = model0(image_tensor)\n",
    "    prediction = logits.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted digit: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
